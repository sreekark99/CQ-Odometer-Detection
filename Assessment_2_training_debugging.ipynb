{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841d0df6-0c34-4fda-82c0-3dbedf3450fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7f77f4-5dc3-450c-b142-f9455753c985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c91568-34d6-452e-a6cb-885c4ccac536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "class OdometerDataset:\n",
    "    def __init__(self, root_folder, output_folder, transform=None):\n",
    "        self.root_folder = root_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.transform = transform\n",
    "        self.data = self.load_annotations()\n",
    "\n",
    "    def load_annotations(self):\n",
    "        data = []  # Initialize an empty list to store image info\n",
    "\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "        for sub_folder in os.listdir(self.root_folder):\n",
    "            sub_folder_path = os.path.join(self.root_folder, sub_folder)\n",
    "            if not os.path.isdir(sub_folder_path):\n",
    "                continue\n",
    "\n",
    "            annotation_file = os.path.join(sub_folder_path, \"via_region_data.json\")\n",
    "            if not os.path.exists(annotation_file):\n",
    "                print(f\"Annotation file missing in {sub_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            with open(annotation_file, 'r') as f:\n",
    "                annotations = json.load(f)\n",
    "\n",
    "            for item in annotations.values():\n",
    "                if 'filename' in item and 'regions' in item:\n",
    "                    image_path = item['filename']\n",
    "                    image_path = os.path.join(sub_folder_path, image_path) if not self.is_url(image_path) else image_path\n",
    "                    \n",
    "                    if not self.is_url(image_path) and not os.path.exists(image_path):\n",
    "                        print(f\"Skipping {image_path}: File not found or invalid.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Process each \"odometer\" region\n",
    "                    for region in item['regions']:\n",
    "                        if region['region_attributes']['identity'] == 'odometer':\n",
    "                            # Get the odometer reading\n",
    "                            odometer_reading = region['region_attributes'].get('reading', 'N/A')\n",
    "                            \n",
    "                            # Add the image info to the data list (excluding unnecessary info)\n",
    "                            image_name = os.path.basename(image_path)\n",
    "                            data.append({\n",
    "                                'image_name': image_name,\n",
    "                                'image_path': image_path,\n",
    "                                'label': odometer_reading\n",
    "                            })\n",
    "\n",
    "        # Convert the list of dictionaries into a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def is_url(self, path):\n",
    "        return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "# Usage example\n",
    "root_folder = pwd + \"\\\\CQ\"  # Set this to your root folder path\n",
    "output_folder = pwd + \"\\\\CQ_OUT\"  # Set this to your output folder path\n",
    "dataset = OdometerDataset(root_folder, output_folder)\n",
    "\n",
    "# The `dataset.data` will now be a DataFrame with columns: image_name, image_path, label\n",
    "df = dataset.data\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc33c10-f7d2-4e80-8849-63a16b1f3492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "class YOLOOdometerDataset:\n",
    "    def __init__(self, root_folder, output_folder, train_ratio=0.8):\n",
    "        self.root_folder = root_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.train_ratio = train_ratio\n",
    "        self.classes = {\"odometer\": 0}  # Class mapping for YOLO\n",
    "        self.prepare_output_folders()\n",
    "        self.data = self.load_annotations()\n",
    "\n",
    "    def prepare_output_folders(self):\n",
    "        # Create YOLO folder structure\n",
    "        for split in ['train', 'val']:\n",
    "            image_dir = os.path.join(self.output_folder, 'images', split)\n",
    "            label_dir = os.path.join(self.output_folder, 'labels', split)\n",
    "            os.makedirs(image_dir, exist_ok=True)\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "\n",
    "    def load_annotations(self):\n",
    "        data = []\n",
    "        all_images = []\n",
    "\n",
    "        for sub_folder in os.listdir(self.root_folder):\n",
    "            sub_folder_path = os.path.join(self.root_folder, sub_folder)\n",
    "            if not os.path.isdir(sub_folder_path):\n",
    "                continue\n",
    "\n",
    "            annotation_file = os.path.join(sub_folder_path, \"via_region_data.json\")\n",
    "            if not os.path.exists(annotation_file):\n",
    "                print(f\"Annotation file missing in {sub_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            with open(annotation_file, 'r') as f:\n",
    "                annotations = json.load(f)\n",
    "\n",
    "            for item in annotations.values():\n",
    "                if 'filename' in item and 'regions' in item:\n",
    "                    image_path = item['filename']\n",
    "                    image = self.load_image(image_path, sub_folder_path)\n",
    "                    if image is None:\n",
    "                        print(f\"Skipping {image_path}: File not found or invalid.\")\n",
    "                        continue\n",
    "                    \n",
    "                    width, height = image.size\n",
    "\n",
    "                    label_content = []\n",
    "                    for region in item['regions']:\n",
    "                        if region['region_attributes'].get('identity') == 'odometer':\n",
    "                            shape = region['shape_attributes']\n",
    "                            points = list(zip(shape['all_points_x'], shape['all_points_y']))\n",
    "                            x_min, y_min = min([point[0] for point in points]), min([point[1] for point in points])\n",
    "                            x_max, y_max = max([point[0] for point in points]), max([point[1] for point in points])\n",
    "\n",
    "                            # Normalize YOLO coordinates\n",
    "                            x_center = ((x_min + x_max) / 2) / width\n",
    "                            y_center = ((y_min + y_max) / 2) / height\n",
    "                            bbox_width = (x_max - x_min) / width\n",
    "                            bbox_height = (y_max - y_min) / height\n",
    "\n",
    "                            label_content.append(f\"{self.classes['odometer']} {x_center} {y_center} {bbox_width} {bbox_height}\")\n",
    "\n",
    "                    # Store image path and label content\n",
    "                    all_images.append((image, label_content, os.path.basename(image_path)))\n",
    "\n",
    "        # Split data into train and val\n",
    "        split_idx = int(len(all_images) * self.train_ratio)\n",
    "        train_data = all_images[:split_idx]\n",
    "        val_data = all_images[split_idx:]\n",
    "\n",
    "        self.save_split(train_data, 'train')\n",
    "        self.save_split(val_data, 'val')\n",
    "\n",
    "    def load_image(self, image_path, sub_folder_path):\n",
    "        \"\"\"Loads an image from a local path or URL.\"\"\"\n",
    "        if self.is_url(image_path):\n",
    "            try:\n",
    "                response = requests.get(image_path, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {image_path}: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            full_path = os.path.join(sub_folder_path, image_path)\n",
    "            if os.path.exists(full_path):\n",
    "                return Image.open(full_path).convert(\"RGB\")\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def is_url(self, path):\n",
    "        \"\"\"Checks if a given path is a URL.\"\"\"\n",
    "        return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "    def save_split(self, data_split, split_name):\n",
    "        for image, label_content, filename in data_split:\n",
    "            # Save image\n",
    "            image_output_path = os.path.join(self.output_folder, 'images', split_name, filename)\n",
    "            image.save(image_output_path)\n",
    "\n",
    "            # Save label\n",
    "            label_output_path = os.path.join(self.output_folder, 'labels', split_name, os.path.splitext(filename)[0] + '.txt')\n",
    "            with open(label_output_path, 'w') as f:\n",
    "                f.write(\"\\n\".join(label_content))\n",
    "\n",
    "# Usage example\n",
    "root_folder = pwd + \"\\\\train\"\n",
    "output_folder = pwd + \"\\\\yolo_custom_stage1_out\"\n",
    "dataset = YOLOOdometerDataset(root_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b718107-6fc8-42de-bc3d-3f78c3320b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python detect.py --weights runs/train/exp3/weights/best.pt --img 1024 --conf-thres 0.5 --iou-thres 0.4 --source ../CQ_CustomTest/'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"python detect.py --weights runs/train/exp3/weights/best.pt --img 1024 --conf-thres 0.5 --iou-thres 0.4 --source ../CQ_CustomTest/ --save-txt\"\"\"\n",
    "#Run from yolov5 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23e0cb11-451b-4809-94ea-b062b80978e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for scraped_13mhov_1654867333607.txt:\n",
      "0 0.269141 0.629167 0.0710938 0.0583333\n",
      "Results for scraped_13OV6N_1654867264673.txt:\n",
      "0 0.517578 0.631944 0.0460938 0.0305556\n",
      "Results for scraped_18a89A_1654867429718.txt:\n",
      "0 0.499609 0.739583 0.0320312 0.0347222\n",
      "Results for scraped_1ck86k_1654867403579.txt:\n",
      "0 0.534375 0.64375 0.05 0.0291667\n",
      "Results for scraped_1G51IL_1654867270622.txt:\n",
      "0 0.539062 0.632639 0.0453125 0.0319444\n",
      "Results for scraped_1h60eA_1654867273398.txt:\n",
      "0 0.55625 0.803472 0.0875 0.0597222\n",
      "Results for scraped_1NdmjE_1654867303557.txt:\n",
      "0 0.496094 0.672917 0.0875 0.0458333\n",
      "Results for scraped_293TcY_1654867359772.txt:\n",
      "0 0.553906 0.622222 0.0578125 0.0361111\n",
      "Results for scraped_2gZeT0_1654867296441.txt:\n",
      "0 0.821094 0.540278 0.0703125 0.0472222\n",
      "Results for scraped_2RvgJ9_1654867423025.txt:\n",
      "0 0.567969 0.488194 0.05 0.0319444\n",
      "Results for scraped_2tEMhE_1654867176535.txt:\n",
      "0 0.786328 0.580556 0.0757812 0.0444444\n",
      "Results for scraped_2VVKar_1654867261897.txt:\n",
      "0 0.301953 0.58125 0.0804688 0.0597222\n",
      "Results for scraped_2y3Jpf_1654867285864.txt:\n",
      "0 0.605469 0.615972 0.0640625 0.0430556\n",
      "Results for scraped_32py82_1654867319865.txt:\n",
      "0 0.884375 0.35625 0.0734375 0.0486111\n",
      "Results for scraped_3gjE2J_1654867287037.txt:\n",
      "0 0.642187 0.725 0.040625 0.0361111\n",
      "Results for scraped_3j3hfG_1654867209727.txt:\n",
      "0 0.810547 0.528472 0.0710938 0.0430556\n",
      "Results for scraped_4KfbPZ_1654867329434.txt:\n",
      "0 0.482812 0.710417 0.0578125 0.0402778\n",
      "Results for scraped_4vjWwU_1654867300783.txt:\n",
      "0 0.604688 0.444444 0.0515625 0.0416667\n",
      "Results for scraped_6ks3TO_1654867260523.txt:\n",
      "0 0.513281 0.498611 0.04375 0.0277778\n",
      "Results for scraped_6S07B1_1654867425382.txt:\n",
      "0 0.535547 0.406944 0.0539063 0.0388889\n",
      "Results for scraped_7e3nZu_1654867189078.txt:\n",
      "0 0.631641 0.489583 0.0617188 0.0430556\n",
      "Results for scraped_7rgfKi_1654867354025.txt:\n",
      "0 0.542969 0.417361 0.05 0.0347222\n",
      "Results for scraped_8IuLPi_1654867205796.txt:\n",
      "0 0.522266 0.65 0.0585938 0.0361111\n",
      "Results for scraped_8OV4sp_1654867361130.txt:\n",
      "0 0.541016 0.569444 0.0492188 0.0361111\n",
      "Results for scraped_8vnu7Z_1654867293923.txt:\n",
      "0 0.507422 0.744444 0.0554687 0.0388889\n",
      "Results for scraped_a9Eo4P_1654867295268.txt:\n",
      "0 0.749219 0.555556 0.0734375 0.0416667\n",
      "Results for scraped_AfAJ0m_1654867311756.txt:\n",
      "0 0.510156 0.795139 0.0609375 0.0430556\n",
      "Results for scraped_afEair_1654867281743.txt:\n",
      "0 0.8 0.524306 0.0890625 0.0625\n",
      "Results for scraped_anN2Jy_1654867375114.txt:\n",
      "0 0.567578 0.398611 0.0632813 0.0388889\n",
      "Results for scraped_aYSwaj_1654867278958.txt:\n",
      "0 0.819141 0.535417 0.0773437 0.0430556\n",
      "Results for scraped_b80bBG_1654867318456.txt:\n",
      "0 0.569141 0.670139 0.0476562 0.0319444\n",
      "Results for scraped_bh8yUz_1654867424399.txt:\n",
      "0 0.817969 0.526389 0.0609375 0.0416667\n",
      "Results for scraped_bxQVCT_1654867196010.txt:\n",
      "0 0.511719 0.779861 0.0640625 0.0402778\n",
      "Results for scraped_C0ALzr_1654867351267.txt:\n",
      "0 0.588281 0.560417 0.071875 0.0319444\n",
      "Results for scraped_C9BKpq_1654867428368.txt:\n",
      "0 0.526563 0.493056 0.0453125 0.0305556\n",
      "Results for scraped_C9prO3_1654867377871.txt:\n",
      "0 0.585156 0.524306 0.053125 0.0375\n",
      "Results for scraped_cDgnYr_1654867314538.txt:\n",
      "0 0.636328 0.438889 0.0820312 0.0527778\n",
      "Results for scraped_cEqY69_1654867367026.txt:\n",
      "0 0.520703 0.7875 0.0554687 0.0361111\n",
      "Results for scraped_cGsUBG_1654867352446.txt:\n",
      "0 0.773047 0.351389 0.0710938 0.0416667\n",
      "Results for scraped_cMNCde_1654867276137.txt:\n",
      "0 0.582422 0.4625 0.0554687 0.0444444\n",
      "Results for scraped_CUwPOL_1654867337701.txt:\n",
      "0 0.496484 0.754861 0.0570313 0.0347222\n",
      "Results for scraped_cwL0wZ_1654867213856.txt:\n",
      "0 0.58125 0.46875 0.05625 0.0347222\n",
      "Results for scraped_dkFKOp_1654867406344.txt:\n",
      "0 0.564844 0.725 0.071875 0.0444444\n",
      "Results for scraped_dlkPtQ_1654867369769.txt:\n",
      "0 0.805859 0.511111 0.0757812 0.0555556\n",
      "Results for scraped_DMk49Q_1654867274779.txt:\n",
      "0 0.596484 0.591667 0.0601562 0.0416667\n",
      "Results for scraped_DOjDOq_1654867347111.txt:\n",
      "0 0.812891 0.509028 0.0804688 0.0513889\n",
      "Results for scraped_DZNPfU_1654867340242.txt:\n",
      "0 0.561719 0.275 0.0609375 0.0444444\n",
      "Results for scraped_e7ExX6_1654867414831.txt:\n",
      "0 0.516406 0.548611 0.03125 0.025\n",
      "Results for scraped_EJtuua_1654867400964.txt:\n",
      "0 0.50625 0.846528 0.065625 0.0375\n",
      "Results for scraped_eUD1cF_1654867388601.txt:\n",
      "0 0.586328 0.672222 0.0742188 0.0388889\n",
      "Results for scraped_EX9mkD_1654867236593.txt:\n",
      "0 0.486719 0.608333 0.065625 0.0361111\n",
      "Results for scraped_eZDFhv_1654867297822.txt:\n",
      "0 0.594922 0.4375 0.0476562 0.0277778\n",
      "Results for scraped_F0ZBtR_1654867288605.txt:\n",
      "0 0.535937 0.560417 0.046875 0.0319444\n",
      "Results for scraped_F57shx_1654867355604.txt:\n",
      "0 0.535156 0.579861 0.065625 0.0347222\n",
      "Results for scraped_f7NaKK_1654867160923.txt:\n",
      "0 0.573438 0.745833 0.0546875 0.0416667\n",
      "Results for scraped_FgE7he_1654867155548.txt:\n",
      "0 0.51875 0.495139 0.0390625 0.0263889\n",
      "Results for scraped_fQZ0IS_1654867341580.txt:\n",
      "0 0.535156 0.710417 0.0734375 0.0402778\n",
      "Results for scraped_fRfxU7_1654867362733.txt:\n",
      "0 0.501562 0.481944 0.059375 0.0361111\n",
      "Results for scraped_fwzLhd_1654867381958.txt:\n",
      "0 0.548828 0.656944 0.0570313 0.0416667\n",
      "Results for scraped_G8H4Bq_1654867380605.txt:\n",
      "0 0.521484 0.589583 0.0617188 0.0402778\n",
      "Results for scraped_gGbRZl_1654867307638.txt:\n",
      "0 0.532422 0.586111 0.0835937 0.05\n",
      "Results for scraped_GNJnMX_1654867230940.txt:\n",
      "0 0.336328 0.59375 0.0804688 0.0597222\n",
      "Results for scraped_gtJQz5_1654867325473.txt:\n",
      "0 0.519922 0.652778 0.0523437 0.0361111\n",
      "Results for scraped_GVxfcD_1654867166680.txt:\n",
      "0 0.582031 0.438194 0.06875 0.0458333\n",
      "Results for scraped_h3FHNv_1654867266259.txt:\n",
      "0 0.580078 0.670139 0.0710938 0.0402778\n",
      "Results for scraped_H9VR19_1654867283116.txt:\n",
      "0 0.524219 0.536111 0.0453125 0.0305556\n",
      "Results for scraped_he2daM_1654867407764.txt:\n",
      "0 0.501157 0.575 0.0740741 0.0583333\n",
      "Results for scraped_hhBH9F_1654867365686.txt:\n",
      "0 0.734766 0.625 0.0539063 0.0305556\n",
      "Results for scraped_HScjj9_1654867344368.txt:\n",
      "0 0.528516 0.598611 0.0601562 0.0361111\n",
      "Results for scraped_I63XWh_1654867313155.txt:\n",
      "0 0.552734 0.633333 0.0710938 0.0388889\n",
      "Results for scraped_InNx9t_1654867396907.txt:\n",
      "0 0.50625 0.395139 0.075 0.0458333\n",
      "Results for scraped_itn0Iu_1654867364280.txt:\n",
      "0 0.737109 0.697222 0.0632813 0.0388889\n",
      "Results for scraped_iZodNw_1654867263282.txt:\n",
      "0 0.514453 0.565278 0.0492188 0.0361111\n",
      "Results for scraped_j1EeaP_1654867386064.txt:\n",
      "0 0.502344 0.506944 0.08125 0.0361111\n",
      "Results for scraped_j4t7I8_1654867413470.txt:\n",
      "0 0.537109 0.61875 0.0523437 0.0347222\n",
      "Results for scraped_J6cy4p_1654867402163.txt:\n",
      "0 0.735547 0.5625 0.0695312 0.0472222\n",
      "Results for scraped_J7xs9D_1654867368423.txt:\n",
      "0 0.513672 0.70625 0.0585938 0.0375\n",
      "Results for scraped_jhwWWL_1654867310391.txt:\n",
      "0 0.585547 0.818056 0.0882813 0.0527778\n",
      "Results for scraped_JkJl2Y_1654867156970.txt:\n",
      "0 0.511719 0.654861 0.0734375 0.0458333\n",
      "Results for scraped_jWEueL_1654867410705.txt:\n",
      "0 0.576563 0.621528 0.0703125 0.0375\n",
      "Results for scraped_JwQ5lB_1654867376500.txt:\n",
      "0 0.738672 0.529861 0.0773437 0.0486111\n",
      "Results for scraped_jY3yuZ_1654867254778.txt:\n",
      "0 0.589453 0.81875 0.0398437 0.0291667\n",
      "Results for scraped_K1SuVb_1654867238155.txt:\n",
      "0 0.796484 0.684028 0.0679687 0.0402778\n",
      "Results for scraped_K5EXOu_1654867224022.txt:\n",
      "0 0.539062 0.765278 0.075 0.0444444\n",
      "Results for scraped_KcEyI3_1654867158160.txt:\n",
      "0 0.782422 0.54375 0.0570313 0.0402778\n",
      "Results for scraped_KMGAN2_1654867358221.txt:\n",
      "0 0.607813 0.668056 0.0390625 0.0277778\n",
      "Results for scraped_Ku22fy_1654867372337.txt:\n",
      "0 0.723437 0.457639 0.05625 0.0375\n",
      "Results for scraped_L4ltEI_1654867165097.txt:\n",
      "0 0.501562 0.472222 0.05 0.0361111\n",
      "Results for scraped_LaekCO_1654867338887.txt:\n",
      "0 0.549219 0.577778 0.053125 0.0361111\n",
      "Results for scraped_lCEui4_1654867392740.txt:\n",
      "0 0.54375 0.500694 0.046875 0.0347222\n",
      "Results for scraped_lfZlWU_1654867292541.txt:\n",
      "0 0.603125 0.55 0.0578125 0.0416667\n",
      "Results for scraped_LJL3sv_1654867315946.txt:\n",
      "0 0.661719 0.561806 0.05625 0.0347222\n",
      "Results for scraped_LKJ66M_1654867180598.txt:\n",
      "0 0.628516 0.565972 0.0445312 0.0319444\n",
      "Results for scraped_llRsmt_1654867398262.txt:\n",
      "0 0.709766 0.532639 0.0632813 0.0430556\n",
      "Results for scraped_Ln2DSK_1654867322692.txt:\n",
      "0 0.598047 0.761806 0.0414063 0.0319444\n",
      "Results for scraped_lQLeMs_1654867309037.txt:\n",
      "0 0.542578 0.402778 0.0554687 0.0361111\n",
      "Results for scraped_LRDISf_1654867226814.txt:\n",
      "0 0.818359 0.504861 0.0726563 0.0541667\n",
      "Results for scraped_m2uGhY_1654867391391.txt:\n",
      "0 0.538281 0.340972 0.065625 0.0430556\n",
      "Results for scraped_mAayFq_1654867349899.txt:\n",
      "0 0.511328 0.673611 0.0632813 0.0472222\n",
      "Results for scraped_mJ9HXN_1654867379261.txt:\n",
      "0 0.507422 0.675694 0.0554687 0.0347222\n",
      "Results for scraped_mjzB5b_1654867348528.txt:\n",
      "0 0.586719 0.676389 0.0375 0.0305556\n",
      "Results for scraped_MPdwRU_1654867247810.txt:\n",
      "0 0.512891 0.4375 0.0445312 0.0305556\n",
      "Results for scraped_MuaIw3_1654867304927.txt:\n",
      "0 0.578516 0.705556 0.0539063 0.0333333\n",
      "Results for scraped_MxKWOH_1654867169396.txt:\n",
      "0 0.627344 0.364583 0.0703125 0.0458333\n",
      "Results for scraped_mYEaGd_1654867198775.txt:\n",
      "0 0.517578 0.611806 0.0757812 0.0458333\n",
      "Results for scraped_nEr1b9_1654867239531.txt:\n",
      "0 0.509375 0.628472 0.046875 0.0319444\n",
      "Results for scraped_nlEw8p_1654867186347.txt:\n",
      "0 0.591797 0.696528 0.0664062 0.0402778\n",
      "Results for scraped_nmFtTR_1654867241132.txt:\n",
      "0 0.605078 0.354861 0.0507812 0.0319444\n",
      "Results for scraped_ntmbIq_1654867306289.txt:\n",
      "0 0.550391 0.802778 0.0726563 0.0388889\n",
      "Results for scraped_nTULQD_1654867181932.txt:\n",
      "0 0.469531 0.604861 0.0859375 0.0375\n",
      "Results for scraped_NwBbPb_1654867269264.txt:\n",
      "0 0.641406 0.641667 0.0453125 0.0361111\n",
      "Results for scraped_O4oilv_1654867299216.txt:\n",
      "0 0.497656 0.592361 0.05 0.0375\n",
      "Results for scraped_O8L3fK_1654867228178.txt:\n",
      "0 0.432422 0.708333 0.0648438 0.0388889\n",
      "Results for scraped_O9tJGO_1654867395537.txt:\n",
      "0 0.488281 0.584028 0.034375 0.0291667\n",
      "Results for scraped_oACa34_1654867302149.txt:\n",
      "0 0.701953 0.638194 0.0632813 0.0347222\n",
      "Results for scraped_OfoNGR_1654867168041.txt:\n",
      "0 0.604297 0.620139 0.0554687 0.0430556\n",
      "Results for scraped_OSQhAR_1654867229536.txt:\n",
      "0 0.553516 0.646528 0.0414063 0.0375\n",
      "Results for scraped_oTESMG_1654867219456.txt:\n",
      "0 0.524609 0.413194 0.0695312 0.0458333\n",
      "Results for scraped_OV9eeQ_1654867384670.txt:\n",
      "0 0.567187 0.643056 0.0546875 0.0388889\n",
      "Results for scraped_P5S28h_1654867173547.txt:\n",
      "0 0.576953 0.483333 0.0570313 0.0416667\n",
      "0 0.577734 0.558333 0.0476562 0.0361111\n",
      "Results for scraped_PBuH8G_1654867373718.txt:\n",
      "0 0.462891 0.864583 0.0585938 0.0402778\n",
      "Results for scraped_PbWvTN_1654867272023.txt:\n",
      "0 0.779688 0.670833 0.0640625 0.0416667\n",
      "Results for scraped_pca196_1654867291334.txt:\n",
      "0 0.725 0.663194 0.0546875 0.0319444\n",
      "Results for scraped_pDvheA_1654867222462.txt:\n",
      "0 0.553516 0.652778 0.0445312 0.0361111\n",
      "Results for scraped_Pj28U7_1654867371163.txt:\n",
      "0 0.481641 0.725694 0.0679687 0.0402778\n",
      "Results for scraped_PlpgBi_1654867284480.txt:\n",
      "0 0.303906 0.653472 0.059375 0.0430556\n",
      "Results for scraped_pMUqvJ_1654867235060.txt:\n",
      "0 0.494141 0.745833 0.0679687 0.0361111\n",
      "Results for scraped_pMY0nl_1654867417541.txt:\n",
      "0 0.457031 0.414583 0.078125 0.0458333\n",
      "Results for scraped_pNzmy5_1654867204261.txt:\n",
      "0 0.559766 0.680556 0.0632813 0.0388889\n",
      "Results for scraped_PsWGBR_1654867326890.txt:\n",
      "0 0.533203 0.597917 0.0554687 0.0402778\n",
      "Results for scraped_pvJMTO_1654867170793.txt:\n",
      "0 0.599219 0.646528 0.0515625 0.0347222\n",
      "Results for scraped_pXs7Vg_1654867324093.txt:\n",
      "0 0.519531 0.594444 0.0625 0.0333333\n",
      "Results for scraped_Q0Qv0E_1654867174954.txt:\n",
      "0 0.565625 0.668056 0.059375 0.0416667\n",
      "Results for scraped_qkwFGb_1654867154136.txt:\n",
      "0 0.523828 0.463889 0.0585938 0.0388889\n",
      "Results for scraped_QQSTzS_1654867225430.txt:\n",
      "0 0.511719 0.55 0.06875 0.0333333\n",
      "Results for scraped_QrCBnx_1654867163701.txt:\n",
      "0 0.480078 0.740278 0.0601562 0.0361111\n",
      "Results for scraped_RAQz9F_1654867334946.txt:\n",
      "0 0.637109 0.418056 0.0601562 0.0388889\n",
      "Results for scraped_rMnpoT_1654867251970.txt:\n",
      "0 0.541797 0.698611 0.0632813 0.0388889\n",
      "Results for scraped_rMTDbQ_1654867211088.txt:\n",
      "0 0.517969 0.607639 0.0703125 0.0347222\n",
      "Results for scraped_RPkyrW_1654867328244.txt:\n",
      "0 0.381641 0.680556 0.0445312 0.0333333\n",
      "Results for scraped_RQAQOE_1654867256186.txt:\n",
      "0 0.584766 0.658333 0.0414063 0.025\n",
      "Results for scraped_s2eAWx_1654867217887.txt:\n",
      "0 0.561719 0.458333 0.0484375 0.0333333\n",
      "Results for scraped_sfFo8o_1654867177919.txt:\n",
      "0 0.589453 0.490972 0.0507812 0.0347222\n",
      "Results for scraped_SGaXXT_1654867244905.txt:\n",
      "0 0.516406 0.391667 0.0671875 0.0444444\n",
      "Results for scraped_Shf2GN_1654867243695.txt:\n",
      "0 0.517969 0.697917 0.0765625 0.0458333\n",
      "Results for scraped_sJ9cOR_1654867277548.txt:\n",
      "0 0.535547 0.394444 0.0585938 0.0361111\n",
      "Results for scraped_sKeTkX_1654867404985.txt:\n",
      "0 0.552734 0.601389 0.0539063 0.0361111\n",
      "Results for scraped_slqT4h_1654867159537.txt:\n",
      "0 0.543359 0.605556 0.0679687 0.0361111\n",
      "Results for scraped_sqXOBp_1654867332231.txt:\n",
      "0 0.545313 0.484722 0.046875 0.0361111\n",
      "Results for scraped_T3ZBFZ_1654867280342.txt:\n",
      "0 0.819922 0.5 0.0585938 0.0388889\n",
      "Results for scraped_T9zGgR_1654867389998.txt:\n",
      "0 0.432031 0.458333 0.065625 0.0472222\n",
      "Results for scraped_tCBIxG_1654867250586.txt:\n",
      "0 0.705469 0.663889 0.053125 0.0333333\n",
      "Results for scraped_tjaH1Y_1654867201416.txt:\n",
      "0 0.493359 0.448611 0.0476562 0.0305556\n",
      "Results for scraped_tVOvEl_1654867249190.txt:\n",
      "0 0.588281 0.50625 0.0546875 0.0347222\n",
      "Results for scraped_twIW6U_1654867259140.txt:\n",
      "0 0.692969 0.775 0.0546875 0.0333333\n",
      "Results for scraped_U0yhlr_1654867183527.txt:\n",
      "0 0.526953 0.545139 0.0445312 0.0291667\n",
      "Results for scraped_ub4Leh_1654867394126.txt:\n",
      "0 0.522266 0.51875 0.0492188 0.0347222\n",
      "Results for scraped_UpVYEL_1654867208384.txt:\n",
      "0 0.506641 0.6 0.0492188 0.0333333\n",
      "Results for scraped_usITWo_1654867172153.txt:\n",
      "0 0.506641 0.638194 0.0601562 0.0375\n",
      "Results for scraped_UtR5wI_1654867387236.txt:\n",
      "0 0.538281 0.480556 0.0515625 0.0305556\n",
      "Results for scraped_UVmQLR_1654867289972.txt:\n",
      "0 0.530078 0.571528 0.0414063 0.0291667\n",
      "Results for scraped_UyA0Mu_1654867193244.txt:\n",
      "0 0.748828 0.597222 0.0648438 0.0444444\n",
      "Results for scraped_Vp71Ls_1654867233658.txt:\n",
      "0 0.506641 0.55 0.0382813 0.0277778\n",
      "Results for scraped_vqQaGC_1654867190467.txt:\n",
      "0 0.532422 0.561806 0.0523437 0.0347222\n",
      "Results for scraped_VwFXNK_1654867342999.txt:\n",
      "0 0.4625 0.513194 0.0421875 0.0263889\n",
      "Results for scraped_WExwqN_1654867421671.txt:\n",
      "0 0.539453 0.49375 0.0757812 0.0513889\n",
      "Results for scraped_WJPTlO_1654867179257.txt:\n",
      "0 0.560547 0.627778 0.0632813 0.0444444\n",
      "Results for scraped_wN2GMF_1654867212456.txt:\n",
      "0 0.605078 0.573611 0.0289063 0.0194444\n",
      "Results for scraped_wSbT81_1654867199988.txt:\n",
      "0 0.559766 0.707639 0.0710938 0.0458333\n",
      "Results for scraped_WsK7wu_1654867416205.txt:\n",
      "0 0.751172 0.571528 0.0867188 0.0541667\n",
      "Results for scraped_WuPwPF_1654867253611.txt:\n",
      "0 0.710938 0.679861 0.065625 0.0402778\n",
      "Results for scraped_wvc2C3_1654867187736.txt:\n",
      "0 0.558203 0.679861 0.0648438 0.0458333\n",
      "Results for scraped_wvkBh3_1654867317117.txt:\n",
      "0 0.453125 0.404861 0.05 0.0319444\n",
      "Results for scraped_wynC5u_1654867246449.txt:\n",
      "0 0.492578 0.539583 0.0382813 0.0236111\n",
      "Results for scraped_x3SRyW_1654867162492.txt:\n",
      "0 0.503516 0.689583 0.0820312 0.0458333\n",
      "Results for scraped_Xci8hW_1654867215126.txt:\n",
      "0 0.532812 0.427083 0.0625 0.0458333\n",
      "Results for scraped_xPCbc8_1654867191814.txt:\n",
      "0 0.722266 0.659722 0.0539063 0.0416667\n",
      "Results for scraped_ycKNvf_1654867412082.txt:\n",
      "0 0.517187 0.829861 0.05625 0.0402778\n",
      "Results for scraped_YIOzqZ_1654867232300.txt:\n",
      "0 0.654297 0.55625 0.0507812 0.0402778\n",
      "Results for scraped_Yybk0D_1654867399610.txt:\n",
      "0 0.636719 0.633333 0.0703125 0.0388889\n",
      "Results for scraped_zi9L6j_1654867194615.txt:\n",
      "0 0.512891 0.550694 0.0398437 0.0263889\n",
      "Results for scraped_zMcuQ1_1654867242510.txt:\n",
      "0 0.569141 0.346528 0.0570313 0.0347222\n",
      "Results for scraped_zr8pPM_1654867336322.txt:\n",
      "0 0.663672 0.693056 0.0507812 0.0305556\n",
      "Results for scraped_zRTbu0_1654867184868.txt:\n",
      "0 0.459766 0.646528 0.0507812 0.0375\n",
      "Results for scraped_Zv4Wfy_1654867257796.txt:\n",
      "0 0.572656 0.719444 0.0578125 0.0361111\n",
      "Results for scraped_ZzLNzk_1654867267672.txt:\n",
      "0 0.532812 0.527083 0.05 0.0347222\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_detection_results(results_folder):\n",
    "    # Get all .txt files in the 'labels' folder\n",
    "    labels_folder = os.path.join(results_folder, \"labels\")\n",
    "    detection_results = {}\n",
    "    \n",
    "    for file_name in os.listdir(labels_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(labels_folder, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                detections = file.readlines()\n",
    "                detection_results[file_name] = detections\n",
    "\n",
    "    return detection_results\n",
    "\n",
    "# Example usage\n",
    "results_folder = pwd + \"\\\\yolov5\\\\runs\\\\detect\\\\exp15\"  # Path to the experiment folder\n",
    "detection_results = read_detection_results(results_folder)\n",
    "\n",
    "# Print results for each image\n",
    "for image_name, detections in detection_results.items():\n",
    "    print(f\"Results for {image_name}:\")\n",
    "    for detection in detections:\n",
    "        print(detection.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b1b1236-c96f-484f-aa0b-2c9d232d0e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered result for scraped_13mhov_1654867333607.txt: ['0', '0.269141', '0.629167', '0.0710938', '0.0583333']\n",
      "Filtered result for scraped_13OV6N_1654867264673.txt: ['0', '0.517578', '0.631944', '0.0460938', '0.0305556']\n",
      "Filtered result for scraped_18a89A_1654867429718.txt: ['0', '0.499609', '0.739583', '0.0320312', '0.0347222']\n",
      "Filtered result for scraped_1ck86k_1654867403579.txt: ['0', '0.534375', '0.64375', '0.05', '0.0291667']\n",
      "Filtered result for scraped_1G51IL_1654867270622.txt: ['0', '0.539062', '0.632639', '0.0453125', '0.0319444']\n",
      "Filtered result for scraped_1h60eA_1654867273398.txt: ['0', '0.55625', '0.803472', '0.0875', '0.0597222']\n",
      "Filtered result for scraped_1NdmjE_1654867303557.txt: ['0', '0.496094', '0.672917', '0.0875', '0.0458333']\n",
      "Filtered result for scraped_293TcY_1654867359772.txt: ['0', '0.553906', '0.622222', '0.0578125', '0.0361111']\n",
      "Filtered result for scraped_2gZeT0_1654867296441.txt: ['0', '0.821094', '0.540278', '0.0703125', '0.0472222']\n",
      "Filtered result for scraped_2RvgJ9_1654867423025.txt: ['0', '0.567969', '0.488194', '0.05', '0.0319444']\n",
      "Filtered result for scraped_2tEMhE_1654867176535.txt: ['0', '0.786328', '0.580556', '0.0757812', '0.0444444']\n",
      "Filtered result for scraped_2VVKar_1654867261897.txt: ['0', '0.301953', '0.58125', '0.0804688', '0.0597222']\n",
      "Filtered result for scraped_2y3Jpf_1654867285864.txt: ['0', '0.605469', '0.615972', '0.0640625', '0.0430556']\n",
      "Filtered result for scraped_32py82_1654867319865.txt: ['0', '0.884375', '0.35625', '0.0734375', '0.0486111']\n",
      "Filtered result for scraped_3gjE2J_1654867287037.txt: ['0', '0.642187', '0.725', '0.040625', '0.0361111']\n",
      "Filtered result for scraped_3j3hfG_1654867209727.txt: ['0', '0.810547', '0.528472', '0.0710938', '0.0430556']\n",
      "Filtered result for scraped_4KfbPZ_1654867329434.txt: ['0', '0.482812', '0.710417', '0.0578125', '0.0402778']\n",
      "Filtered result for scraped_4vjWwU_1654867300783.txt: ['0', '0.604688', '0.444444', '0.0515625', '0.0416667']\n",
      "Filtered result for scraped_6ks3TO_1654867260523.txt: ['0', '0.513281', '0.498611', '0.04375', '0.0277778']\n",
      "Filtered result for scraped_6S07B1_1654867425382.txt: ['0', '0.535547', '0.406944', '0.0539063', '0.0388889']\n",
      "Filtered result for scraped_7e3nZu_1654867189078.txt: ['0', '0.631641', '0.489583', '0.0617188', '0.0430556']\n",
      "Filtered result for scraped_7rgfKi_1654867354025.txt: ['0', '0.542969', '0.417361', '0.05', '0.0347222']\n",
      "Filtered result for scraped_8IuLPi_1654867205796.txt: ['0', '0.522266', '0.65', '0.0585938', '0.0361111']\n",
      "Filtered result for scraped_8OV4sp_1654867361130.txt: ['0', '0.541016', '0.569444', '0.0492188', '0.0361111']\n",
      "Filtered result for scraped_8vnu7Z_1654867293923.txt: ['0', '0.507422', '0.744444', '0.0554687', '0.0388889']\n",
      "Filtered result for scraped_a9Eo4P_1654867295268.txt: ['0', '0.749219', '0.555556', '0.0734375', '0.0416667']\n",
      "Filtered result for scraped_AfAJ0m_1654867311756.txt: ['0', '0.510156', '0.795139', '0.0609375', '0.0430556']\n",
      "Filtered result for scraped_afEair_1654867281743.txt: ['0', '0.8', '0.524306', '0.0890625', '0.0625']\n",
      "Filtered result for scraped_anN2Jy_1654867375114.txt: ['0', '0.567578', '0.398611', '0.0632813', '0.0388889']\n",
      "Filtered result for scraped_aYSwaj_1654867278958.txt: ['0', '0.819141', '0.535417', '0.0773437', '0.0430556']\n",
      "Filtered result for scraped_b80bBG_1654867318456.txt: ['0', '0.569141', '0.670139', '0.0476562', '0.0319444']\n",
      "Filtered result for scraped_bh8yUz_1654867424399.txt: ['0', '0.817969', '0.526389', '0.0609375', '0.0416667']\n",
      "Filtered result for scraped_bxQVCT_1654867196010.txt: ['0', '0.511719', '0.779861', '0.0640625', '0.0402778']\n",
      "Filtered result for scraped_C0ALzr_1654867351267.txt: ['0', '0.588281', '0.560417', '0.071875', '0.0319444']\n",
      "Filtered result for scraped_C9BKpq_1654867428368.txt: ['0', '0.526563', '0.493056', '0.0453125', '0.0305556']\n",
      "Filtered result for scraped_C9prO3_1654867377871.txt: ['0', '0.585156', '0.524306', '0.053125', '0.0375']\n",
      "Filtered result for scraped_cDgnYr_1654867314538.txt: ['0', '0.636328', '0.438889', '0.0820312', '0.0527778']\n",
      "Filtered result for scraped_cEqY69_1654867367026.txt: ['0', '0.520703', '0.7875', '0.0554687', '0.0361111']\n",
      "Filtered result for scraped_cGsUBG_1654867352446.txt: ['0', '0.773047', '0.351389', '0.0710938', '0.0416667']\n",
      "Filtered result for scraped_cMNCde_1654867276137.txt: ['0', '0.582422', '0.4625', '0.0554687', '0.0444444']\n",
      "Filtered result for scraped_CUwPOL_1654867337701.txt: ['0', '0.496484', '0.754861', '0.0570313', '0.0347222']\n",
      "Filtered result for scraped_cwL0wZ_1654867213856.txt: ['0', '0.58125', '0.46875', '0.05625', '0.0347222']\n",
      "Filtered result for scraped_dkFKOp_1654867406344.txt: ['0', '0.564844', '0.725', '0.071875', '0.0444444']\n",
      "Filtered result for scraped_dlkPtQ_1654867369769.txt: ['0', '0.805859', '0.511111', '0.0757812', '0.0555556']\n",
      "Filtered result for scraped_DMk49Q_1654867274779.txt: ['0', '0.596484', '0.591667', '0.0601562', '0.0416667']\n",
      "Filtered result for scraped_DOjDOq_1654867347111.txt: ['0', '0.812891', '0.509028', '0.0804688', '0.0513889']\n",
      "Filtered result for scraped_DZNPfU_1654867340242.txt: ['0', '0.561719', '0.275', '0.0609375', '0.0444444']\n",
      "Filtered result for scraped_e7ExX6_1654867414831.txt: ['0', '0.516406', '0.548611', '0.03125', '0.025']\n",
      "Filtered result for scraped_EJtuua_1654867400964.txt: ['0', '0.50625', '0.846528', '0.065625', '0.0375']\n",
      "Filtered result for scraped_eUD1cF_1654867388601.txt: ['0', '0.586328', '0.672222', '0.0742188', '0.0388889']\n",
      "Filtered result for scraped_EX9mkD_1654867236593.txt: ['0', '0.486719', '0.608333', '0.065625', '0.0361111']\n",
      "Filtered result for scraped_eZDFhv_1654867297822.txt: ['0', '0.594922', '0.4375', '0.0476562', '0.0277778']\n",
      "Filtered result for scraped_F0ZBtR_1654867288605.txt: ['0', '0.535937', '0.560417', '0.046875', '0.0319444']\n",
      "Filtered result for scraped_F57shx_1654867355604.txt: ['0', '0.535156', '0.579861', '0.065625', '0.0347222']\n",
      "Filtered result for scraped_f7NaKK_1654867160923.txt: ['0', '0.573438', '0.745833', '0.0546875', '0.0416667']\n",
      "Filtered result for scraped_FgE7he_1654867155548.txt: ['0', '0.51875', '0.495139', '0.0390625', '0.0263889']\n",
      "Filtered result for scraped_fQZ0IS_1654867341580.txt: ['0', '0.535156', '0.710417', '0.0734375', '0.0402778']\n",
      "Filtered result for scraped_fRfxU7_1654867362733.txt: ['0', '0.501562', '0.481944', '0.059375', '0.0361111']\n",
      "Filtered result for scraped_fwzLhd_1654867381958.txt: ['0', '0.548828', '0.656944', '0.0570313', '0.0416667']\n",
      "Filtered result for scraped_G8H4Bq_1654867380605.txt: ['0', '0.521484', '0.589583', '0.0617188', '0.0402778']\n",
      "Filtered result for scraped_gGbRZl_1654867307638.txt: ['0', '0.532422', '0.586111', '0.0835937', '0.05']\n",
      "Filtered result for scraped_GNJnMX_1654867230940.txt: ['0', '0.336328', '0.59375', '0.0804688', '0.0597222']\n",
      "Filtered result for scraped_gtJQz5_1654867325473.txt: ['0', '0.519922', '0.652778', '0.0523437', '0.0361111']\n",
      "Filtered result for scraped_GVxfcD_1654867166680.txt: ['0', '0.582031', '0.438194', '0.06875', '0.0458333']\n",
      "Filtered result for scraped_h3FHNv_1654867266259.txt: ['0', '0.580078', '0.670139', '0.0710938', '0.0402778']\n",
      "Filtered result for scraped_H9VR19_1654867283116.txt: ['0', '0.524219', '0.536111', '0.0453125', '0.0305556']\n",
      "Filtered result for scraped_he2daM_1654867407764.txt: ['0', '0.501157', '0.575', '0.0740741', '0.0583333']\n",
      "Filtered result for scraped_hhBH9F_1654867365686.txt: ['0', '0.734766', '0.625', '0.0539063', '0.0305556']\n",
      "Filtered result for scraped_HScjj9_1654867344368.txt: ['0', '0.528516', '0.598611', '0.0601562', '0.0361111']\n",
      "Filtered result for scraped_I63XWh_1654867313155.txt: ['0', '0.552734', '0.633333', '0.0710938', '0.0388889']\n",
      "Filtered result for scraped_InNx9t_1654867396907.txt: ['0', '0.50625', '0.395139', '0.075', '0.0458333']\n",
      "Filtered result for scraped_itn0Iu_1654867364280.txt: ['0', '0.737109', '0.697222', '0.0632813', '0.0388889']\n",
      "Filtered result for scraped_iZodNw_1654867263282.txt: ['0', '0.514453', '0.565278', '0.0492188', '0.0361111']\n",
      "Filtered result for scraped_j1EeaP_1654867386064.txt: ['0', '0.502344', '0.506944', '0.08125', '0.0361111']\n",
      "Filtered result for scraped_j4t7I8_1654867413470.txt: ['0', '0.537109', '0.61875', '0.0523437', '0.0347222']\n",
      "Filtered result for scraped_J6cy4p_1654867402163.txt: ['0', '0.735547', '0.5625', '0.0695312', '0.0472222']\n",
      "Filtered result for scraped_J7xs9D_1654867368423.txt: ['0', '0.513672', '0.70625', '0.0585938', '0.0375']\n",
      "Filtered result for scraped_jhwWWL_1654867310391.txt: ['0', '0.585547', '0.818056', '0.0882813', '0.0527778']\n",
      "Filtered result for scraped_JkJl2Y_1654867156970.txt: ['0', '0.511719', '0.654861', '0.0734375', '0.0458333']\n",
      "Filtered result for scraped_jWEueL_1654867410705.txt: ['0', '0.576563', '0.621528', '0.0703125', '0.0375']\n",
      "Filtered result for scraped_JwQ5lB_1654867376500.txt: ['0', '0.738672', '0.529861', '0.0773437', '0.0486111']\n",
      "Filtered result for scraped_jY3yuZ_1654867254778.txt: ['0', '0.589453', '0.81875', '0.0398437', '0.0291667']\n",
      "Filtered result for scraped_K1SuVb_1654867238155.txt: ['0', '0.796484', '0.684028', '0.0679687', '0.0402778']\n",
      "Filtered result for scraped_K5EXOu_1654867224022.txt: ['0', '0.539062', '0.765278', '0.075', '0.0444444']\n",
      "Filtered result for scraped_KcEyI3_1654867158160.txt: ['0', '0.782422', '0.54375', '0.0570313', '0.0402778']\n",
      "Filtered result for scraped_KMGAN2_1654867358221.txt: ['0', '0.607813', '0.668056', '0.0390625', '0.0277778']\n",
      "Filtered result for scraped_Ku22fy_1654867372337.txt: ['0', '0.723437', '0.457639', '0.05625', '0.0375']\n",
      "Filtered result for scraped_L4ltEI_1654867165097.txt: ['0', '0.501562', '0.472222', '0.05', '0.0361111']\n",
      "Filtered result for scraped_LaekCO_1654867338887.txt: ['0', '0.549219', '0.577778', '0.053125', '0.0361111']\n",
      "Filtered result for scraped_lCEui4_1654867392740.txt: ['0', '0.54375', '0.500694', '0.046875', '0.0347222']\n",
      "Filtered result for scraped_lfZlWU_1654867292541.txt: ['0', '0.603125', '0.55', '0.0578125', '0.0416667']\n",
      "Filtered result for scraped_LJL3sv_1654867315946.txt: ['0', '0.661719', '0.561806', '0.05625', '0.0347222']\n",
      "Filtered result for scraped_LKJ66M_1654867180598.txt: ['0', '0.628516', '0.565972', '0.0445312', '0.0319444']\n",
      "Filtered result for scraped_llRsmt_1654867398262.txt: ['0', '0.709766', '0.532639', '0.0632813', '0.0430556']\n",
      "Filtered result for scraped_Ln2DSK_1654867322692.txt: ['0', '0.598047', '0.761806', '0.0414063', '0.0319444']\n",
      "Filtered result for scraped_lQLeMs_1654867309037.txt: ['0', '0.542578', '0.402778', '0.0554687', '0.0361111']\n",
      "Filtered result for scraped_LRDISf_1654867226814.txt: ['0', '0.818359', '0.504861', '0.0726563', '0.0541667']\n",
      "Filtered result for scraped_m2uGhY_1654867391391.txt: ['0', '0.538281', '0.340972', '0.065625', '0.0430556']\n",
      "Filtered result for scraped_mAayFq_1654867349899.txt: ['0', '0.511328', '0.673611', '0.0632813', '0.0472222']\n",
      "Filtered result for scraped_mJ9HXN_1654867379261.txt: ['0', '0.507422', '0.675694', '0.0554687', '0.0347222']\n",
      "Filtered result for scraped_mjzB5b_1654867348528.txt: ['0', '0.586719', '0.676389', '0.0375', '0.0305556']\n",
      "Filtered result for scraped_MPdwRU_1654867247810.txt: ['0', '0.512891', '0.4375', '0.0445312', '0.0305556']\n",
      "Filtered result for scraped_MuaIw3_1654867304927.txt: ['0', '0.578516', '0.705556', '0.0539063', '0.0333333']\n",
      "Filtered result for scraped_MxKWOH_1654867169396.txt: ['0', '0.627344', '0.364583', '0.0703125', '0.0458333']\n",
      "Filtered result for scraped_mYEaGd_1654867198775.txt: ['0', '0.517578', '0.611806', '0.0757812', '0.0458333']\n",
      "Filtered result for scraped_nEr1b9_1654867239531.txt: ['0', '0.509375', '0.628472', '0.046875', '0.0319444']\n",
      "Filtered result for scraped_nlEw8p_1654867186347.txt: ['0', '0.591797', '0.696528', '0.0664062', '0.0402778']\n",
      "Filtered result for scraped_nmFtTR_1654867241132.txt: ['0', '0.605078', '0.354861', '0.0507812', '0.0319444']\n",
      "Filtered result for scraped_ntmbIq_1654867306289.txt: ['0', '0.550391', '0.802778', '0.0726563', '0.0388889']\n",
      "Filtered result for scraped_nTULQD_1654867181932.txt: ['0', '0.469531', '0.604861', '0.0859375', '0.0375']\n",
      "Filtered result for scraped_NwBbPb_1654867269264.txt: ['0', '0.641406', '0.641667', '0.0453125', '0.0361111']\n",
      "Filtered result for scraped_O4oilv_1654867299216.txt: ['0', '0.497656', '0.592361', '0.05', '0.0375']\n",
      "Filtered result for scraped_O8L3fK_1654867228178.txt: ['0', '0.432422', '0.708333', '0.0648438', '0.0388889']\n",
      "Filtered result for scraped_O9tJGO_1654867395537.txt: ['0', '0.488281', '0.584028', '0.034375', '0.0291667']\n",
      "Filtered result for scraped_oACa34_1654867302149.txt: ['0', '0.701953', '0.638194', '0.0632813', '0.0347222']\n",
      "Filtered result for scraped_OfoNGR_1654867168041.txt: ['0', '0.604297', '0.620139', '0.0554687', '0.0430556']\n",
      "Filtered result for scraped_OSQhAR_1654867229536.txt: ['0', '0.553516', '0.646528', '0.0414063', '0.0375']\n",
      "Filtered result for scraped_oTESMG_1654867219456.txt: ['0', '0.524609', '0.413194', '0.0695312', '0.0458333']\n",
      "Filtered result for scraped_OV9eeQ_1654867384670.txt: ['0', '0.567187', '0.643056', '0.0546875', '0.0388889']\n",
      "Filtered result for scraped_P5S28h_1654867173547.txt: ['0', '0.576953', '0.483333', '0.0570313', '0.0416667']\n",
      "Filtered result for scraped_PBuH8G_1654867373718.txt: ['0', '0.462891', '0.864583', '0.0585938', '0.0402778']\n",
      "Filtered result for scraped_PbWvTN_1654867272023.txt: ['0', '0.779688', '0.670833', '0.0640625', '0.0416667']\n",
      "Filtered result for scraped_pca196_1654867291334.txt: ['0', '0.725', '0.663194', '0.0546875', '0.0319444']\n",
      "Filtered result for scraped_pDvheA_1654867222462.txt: ['0', '0.553516', '0.652778', '0.0445312', '0.0361111']\n",
      "Filtered result for scraped_Pj28U7_1654867371163.txt: ['0', '0.481641', '0.725694', '0.0679687', '0.0402778']\n",
      "Filtered result for scraped_PlpgBi_1654867284480.txt: ['0', '0.303906', '0.653472', '0.059375', '0.0430556']\n",
      "Filtered result for scraped_pMUqvJ_1654867235060.txt: ['0', '0.494141', '0.745833', '0.0679687', '0.0361111']\n",
      "Filtered result for scraped_pMY0nl_1654867417541.txt: ['0', '0.457031', '0.414583', '0.078125', '0.0458333']\n",
      "Filtered result for scraped_pNzmy5_1654867204261.txt: ['0', '0.559766', '0.680556', '0.0632813', '0.0388889']\n",
      "Filtered result for scraped_PsWGBR_1654867326890.txt: ['0', '0.533203', '0.597917', '0.0554687', '0.0402778']\n",
      "Filtered result for scraped_pvJMTO_1654867170793.txt: ['0', '0.599219', '0.646528', '0.0515625', '0.0347222']\n",
      "Filtered result for scraped_pXs7Vg_1654867324093.txt: ['0', '0.519531', '0.594444', '0.0625', '0.0333333']\n",
      "Filtered result for scraped_Q0Qv0E_1654867174954.txt: ['0', '0.565625', '0.668056', '0.059375', '0.0416667']\n",
      "Filtered result for scraped_qkwFGb_1654867154136.txt: ['0', '0.523828', '0.463889', '0.0585938', '0.0388889']\n",
      "Filtered result for scraped_QQSTzS_1654867225430.txt: ['0', '0.511719', '0.55', '0.06875', '0.0333333']\n",
      "Filtered result for scraped_QrCBnx_1654867163701.txt: ['0', '0.480078', '0.740278', '0.0601562', '0.0361111']\n",
      "Filtered result for scraped_RAQz9F_1654867334946.txt: ['0', '0.637109', '0.418056', '0.0601562', '0.0388889']\n",
      "Filtered result for scraped_rMnpoT_1654867251970.txt: ['0', '0.541797', '0.698611', '0.0632813', '0.0388889']\n",
      "Filtered result for scraped_rMTDbQ_1654867211088.txt: ['0', '0.517969', '0.607639', '0.0703125', '0.0347222']\n",
      "Filtered result for scraped_RPkyrW_1654867328244.txt: ['0', '0.381641', '0.680556', '0.0445312', '0.0333333']\n",
      "Filtered result for scraped_RQAQOE_1654867256186.txt: ['0', '0.584766', '0.658333', '0.0414063', '0.025']\n",
      "Filtered result for scraped_s2eAWx_1654867217887.txt: ['0', '0.561719', '0.458333', '0.0484375', '0.0333333']\n",
      "Filtered result for scraped_sfFo8o_1654867177919.txt: ['0', '0.589453', '0.490972', '0.0507812', '0.0347222']\n",
      "Filtered result for scraped_SGaXXT_1654867244905.txt: ['0', '0.516406', '0.391667', '0.0671875', '0.0444444']\n",
      "Filtered result for scraped_Shf2GN_1654867243695.txt: ['0', '0.517969', '0.697917', '0.0765625', '0.0458333']\n",
      "Filtered result for scraped_sJ9cOR_1654867277548.txt: ['0', '0.535547', '0.394444', '0.0585938', '0.0361111']\n",
      "Filtered result for scraped_sKeTkX_1654867404985.txt: ['0', '0.552734', '0.601389', '0.0539063', '0.0361111']\n",
      "Filtered result for scraped_slqT4h_1654867159537.txt: ['0', '0.543359', '0.605556', '0.0679687', '0.0361111']\n",
      "Filtered result for scraped_sqXOBp_1654867332231.txt: ['0', '0.545313', '0.484722', '0.046875', '0.0361111']\n",
      "Filtered result for scraped_T3ZBFZ_1654867280342.txt: ['0', '0.819922', '0.5', '0.0585938', '0.0388889']\n",
      "Filtered result for scraped_T9zGgR_1654867389998.txt: ['0', '0.432031', '0.458333', '0.065625', '0.0472222']\n",
      "Filtered result for scraped_tCBIxG_1654867250586.txt: ['0', '0.705469', '0.663889', '0.053125', '0.0333333']\n",
      "Filtered result for scraped_tjaH1Y_1654867201416.txt: ['0', '0.493359', '0.448611', '0.0476562', '0.0305556']\n",
      "Filtered result for scraped_tVOvEl_1654867249190.txt: ['0', '0.588281', '0.50625', '0.0546875', '0.0347222']\n",
      "Filtered result for scraped_twIW6U_1654867259140.txt: ['0', '0.692969', '0.775', '0.0546875', '0.0333333']\n",
      "Filtered result for scraped_U0yhlr_1654867183527.txt: ['0', '0.526953', '0.545139', '0.0445312', '0.0291667']\n",
      "Filtered result for scraped_ub4Leh_1654867394126.txt: ['0', '0.522266', '0.51875', '0.0492188', '0.0347222']\n",
      "Filtered result for scraped_UpVYEL_1654867208384.txt: ['0', '0.506641', '0.6', '0.0492188', '0.0333333']\n",
      "Filtered result for scraped_usITWo_1654867172153.txt: ['0', '0.506641', '0.638194', '0.0601562', '0.0375']\n",
      "Filtered result for scraped_UtR5wI_1654867387236.txt: ['0', '0.538281', '0.480556', '0.0515625', '0.0305556']\n",
      "Filtered result for scraped_UVmQLR_1654867289972.txt: ['0', '0.530078', '0.571528', '0.0414063', '0.0291667']\n",
      "Filtered result for scraped_UyA0Mu_1654867193244.txt: ['0', '0.748828', '0.597222', '0.0648438', '0.0444444']\n",
      "Filtered result for scraped_Vp71Ls_1654867233658.txt: ['0', '0.506641', '0.55', '0.0382813', '0.0277778']\n",
      "Filtered result for scraped_vqQaGC_1654867190467.txt: ['0', '0.532422', '0.561806', '0.0523437', '0.0347222']\n",
      "Filtered result for scraped_VwFXNK_1654867342999.txt: ['0', '0.4625', '0.513194', '0.0421875', '0.0263889']\n",
      "Filtered result for scraped_WExwqN_1654867421671.txt: ['0', '0.539453', '0.49375', '0.0757812', '0.0513889']\n",
      "Filtered result for scraped_WJPTlO_1654867179257.txt: ['0', '0.560547', '0.627778', '0.0632813', '0.0444444']\n",
      "Filtered result for scraped_wN2GMF_1654867212456.txt: ['0', '0.605078', '0.573611', '0.0289063', '0.0194444']\n",
      "Filtered result for scraped_wSbT81_1654867199988.txt: ['0', '0.559766', '0.707639', '0.0710938', '0.0458333']\n",
      "Filtered result for scraped_WsK7wu_1654867416205.txt: ['0', '0.751172', '0.571528', '0.0867188', '0.0541667']\n",
      "Filtered result for scraped_WuPwPF_1654867253611.txt: ['0', '0.710938', '0.679861', '0.065625', '0.0402778']\n",
      "Filtered result for scraped_wvc2C3_1654867187736.txt: ['0', '0.558203', '0.679861', '0.0648438', '0.0458333']\n",
      "Filtered result for scraped_wvkBh3_1654867317117.txt: ['0', '0.453125', '0.404861', '0.05', '0.0319444']\n",
      "Filtered result for scraped_wynC5u_1654867246449.txt: ['0', '0.492578', '0.539583', '0.0382813', '0.0236111']\n",
      "Filtered result for scraped_x3SRyW_1654867162492.txt: ['0', '0.503516', '0.689583', '0.0820312', '0.0458333']\n",
      "Filtered result for scraped_Xci8hW_1654867215126.txt: ['0', '0.532812', '0.427083', '0.0625', '0.0458333']\n",
      "Filtered result for scraped_xPCbc8_1654867191814.txt: ['0', '0.722266', '0.659722', '0.0539063', '0.0416667']\n",
      "Filtered result for scraped_ycKNvf_1654867412082.txt: ['0', '0.517187', '0.829861', '0.05625', '0.0402778']\n",
      "Filtered result for scraped_YIOzqZ_1654867232300.txt: ['0', '0.654297', '0.55625', '0.0507812', '0.0402778']\n",
      "Filtered result for scraped_Yybk0D_1654867399610.txt: ['0', '0.636719', '0.633333', '0.0703125', '0.0388889']\n",
      "Filtered result for scraped_zi9L6j_1654867194615.txt: ['0', '0.512891', '0.550694', '0.0398437', '0.0263889']\n",
      "Filtered result for scraped_zMcuQ1_1654867242510.txt: ['0', '0.569141', '0.346528', '0.0570313', '0.0347222']\n",
      "Filtered result for scraped_zr8pPM_1654867336322.txt: ['0', '0.663672', '0.693056', '0.0507812', '0.0305556']\n",
      "Filtered result for scraped_zRTbu0_1654867184868.txt: ['0', '0.459766', '0.646528', '0.0507812', '0.0375']\n",
      "Filtered result for scraped_Zv4Wfy_1654867257796.txt: ['0', '0.572656', '0.719444', '0.0578125', '0.0361111']\n",
      "Filtered result for scraped_ZzLNzk_1654867267672.txt: ['0', '0.532812', '0.527083', '0.05', '0.0347222']\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_and_filter_detections(results_folder):\n",
    "    labels_folder = os.path.join(results_folder, \"labels\")\n",
    "    filtered_results = {}\n",
    "\n",
    "    # Process each detection result file\n",
    "    for file_name in os.listdir(labels_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(labels_folder, file_name)\n",
    "            with open(file_path, 'r') as file:\n",
    "                detections = file.readlines()\n",
    "                \n",
    "                # Parse detections and sort by confidence (last element in each line)\n",
    "                detections = [line.strip().split() for line in detections]\n",
    "                detections = sorted(detections, key=lambda x: float(x[-1]), reverse=True)\n",
    "\n",
    "                # Keep only the detection with the highest confidence\n",
    "                best_detection = detections[0] if detections else None\n",
    "\n",
    "                if best_detection:\n",
    "                    filtered_results[file_name] = best_detection\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "# Example usage\n",
    "results_folder = pwd + \"\\\\yolov5\\\\runs\\\\detect\\\\exp15\"\n",
    "filtered_results = read_and_filter_detections(results_folder)\n",
    "\n",
    "# Print filtered results\n",
    "for image_name, detection in filtered_results.items():\n",
    "    print(f\"Filtered result for {image_name}: {detection}\")\n",
    "print(len(filtered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05a5239d-6138-46d6-b433-62e67620bb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_13mhov_1654867333607.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_13OV6N_1654867264673.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_18a89A_1654867429718.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_1ck86k_1654867403579.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_1G51IL_1654867270622.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_1h60eA_1654867273398.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_1NdmjE_1654867303557.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_293TcY_1654867359772.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_2gZeT0_1654867296441.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_2RvgJ9_1654867423025.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_2tEMhE_1654867176535.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_2VVKar_1654867261897.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_2y3Jpf_1654867285864.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_32py82_1654867319865.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_3gjE2J_1654867287037.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_3j3hfG_1654867209727.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_4KfbPZ_1654867329434.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_4vjWwU_1654867300783.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_6ks3TO_1654867260523.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_6S07B1_1654867425382.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_7e3nZu_1654867189078.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_7rgfKi_1654867354025.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_8IuLPi_1654867205796.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_8OV4sp_1654867361130.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_8vnu7Z_1654867293923.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_a9Eo4P_1654867295268.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_AfAJ0m_1654867311756.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_afEair_1654867281743.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_anN2Jy_1654867375114.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_aYSwaj_1654867278958.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_b80bBG_1654867318456.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_bh8yUz_1654867424399.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_bxQVCT_1654867196010.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_C0ALzr_1654867351267.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_C9BKpq_1654867428368.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_C9prO3_1654867377871.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_cDgnYr_1654867314538.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_cEqY69_1654867367026.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_cGsUBG_1654867352446.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_cMNCde_1654867276137.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_CUwPOL_1654867337701.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_cwL0wZ_1654867213856.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_dkFKOp_1654867406344.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_dlkPtQ_1654867369769.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_DMk49Q_1654867274779.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_DOjDOq_1654867347111.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_DZNPfU_1654867340242.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_e7ExX6_1654867414831.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_EJtuua_1654867400964.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_eUD1cF_1654867388601.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_EX9mkD_1654867236593.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_eZDFhv_1654867297822.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_F0ZBtR_1654867288605.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_F57shx_1654867355604.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_f7NaKK_1654867160923.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_FgE7he_1654867155548.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_fQZ0IS_1654867341580.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_fRfxU7_1654867362733.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_fwzLhd_1654867381958.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_G8H4Bq_1654867380605.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_gGbRZl_1654867307638.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_GNJnMX_1654867230940.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_gtJQz5_1654867325473.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_GVxfcD_1654867166680.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_h3FHNv_1654867266259.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_H9VR19_1654867283116.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_he2daM_1654867407764.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_hhBH9F_1654867365686.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_HScjj9_1654867344368.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_I63XWh_1654867313155.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_InNx9t_1654867396907.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_itn0Iu_1654867364280.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_iZodNw_1654867263282.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_j1EeaP_1654867386064.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_j4t7I8_1654867413470.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_J6cy4p_1654867402163.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_J7xs9D_1654867368423.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_jhwWWL_1654867310391.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_JkJl2Y_1654867156970.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_jWEueL_1654867410705.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_JwQ5lB_1654867376500.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_jY3yuZ_1654867254778.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_K1SuVb_1654867238155.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_K5EXOu_1654867224022.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_KcEyI3_1654867158160.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_KMGAN2_1654867358221.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Ku22fy_1654867372337.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_L4ltEI_1654867165097.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_LaekCO_1654867338887.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_lCEui4_1654867392740.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_lfZlWU_1654867292541.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_LJL3sv_1654867315946.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_LKJ66M_1654867180598.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_llRsmt_1654867398262.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Ln2DSK_1654867322692.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_lQLeMs_1654867309037.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_LRDISf_1654867226814.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_m2uGhY_1654867391391.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_mAayFq_1654867349899.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_mJ9HXN_1654867379261.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_mjzB5b_1654867348528.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_MPdwRU_1654867247810.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_MuaIw3_1654867304927.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_MxKWOH_1654867169396.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_mYEaGd_1654867198775.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_nEr1b9_1654867239531.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_nlEw8p_1654867186347.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_nmFtTR_1654867241132.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_ntmbIq_1654867306289.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_nTULQD_1654867181932.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_NwBbPb_1654867269264.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_O4oilv_1654867299216.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_O8L3fK_1654867228178.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_O9tJGO_1654867395537.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_oACa34_1654867302149.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_OfoNGR_1654867168041.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_OSQhAR_1654867229536.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_oTESMG_1654867219456.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_OV9eeQ_1654867384670.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_P5S28h_1654867173547.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_PBuH8G_1654867373718.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_PbWvTN_1654867272023.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pca196_1654867291334.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pDvheA_1654867222462.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Pj28U7_1654867371163.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_PlpgBi_1654867284480.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pMUqvJ_1654867235060.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pMY0nl_1654867417541.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pNzmy5_1654867204261.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_PsWGBR_1654867326890.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pvJMTO_1654867170793.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_pXs7Vg_1654867324093.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Q0Qv0E_1654867174954.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_qkwFGb_1654867154136.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_QQSTzS_1654867225430.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_QrCBnx_1654867163701.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_RAQz9F_1654867334946.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_rMnpoT_1654867251970.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_rMTDbQ_1654867211088.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_RPkyrW_1654867328244.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_RQAQOE_1654867256186.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_s2eAWx_1654867217887.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_sfFo8o_1654867177919.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_SGaXXT_1654867244905.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Shf2GN_1654867243695.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_sJ9cOR_1654867277548.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_sKeTkX_1654867404985.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_slqT4h_1654867159537.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_sqXOBp_1654867332231.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_T3ZBFZ_1654867280342.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_T9zGgR_1654867389998.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_tCBIxG_1654867250586.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_tjaH1Y_1654867201416.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_tVOvEl_1654867249190.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_twIW6U_1654867259140.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_U0yhlr_1654867183527.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_ub4Leh_1654867394126.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_UpVYEL_1654867208384.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_usITWo_1654867172153.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_UtR5wI_1654867387236.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_UVmQLR_1654867289972.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_UyA0Mu_1654867193244.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Vp71Ls_1654867233658.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_vqQaGC_1654867190467.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_VwFXNK_1654867342999.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_WExwqN_1654867421671.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_WJPTlO_1654867179257.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_wN2GMF_1654867212456.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_wSbT81_1654867199988.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_WsK7wu_1654867416205.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_WuPwPF_1654867253611.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_wvc2C3_1654867187736.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_wvkBh3_1654867317117.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_wynC5u_1654867246449.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_x3SRyW_1654867162492.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Xci8hW_1654867215126.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_xPCbc8_1654867191814.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_ycKNvf_1654867412082.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_YIOzqZ_1654867232300.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Yybk0D_1654867399610.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_zi9L6j_1654867194615.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_zMcuQ1_1654867242510.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_zr8pPM_1654867336322.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_zRTbu0_1654867184868.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_Zv4Wfy_1654867257796.jpg\n",
      "Cropped image saved: C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\CQ_CustomTest_cropped\\cropped_scraped_ZzLNzk_1654867267672.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def crop_odometer_regions(results_folder, images_folder, output_folder):\n",
    "    labels_folder = os.path.join(results_folder, \"labels\")\n",
    "    filtered_results = {}\n",
    "\n",
    "    # Create output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Process each detection result file\n",
    "    for file_name in os.listdir(labels_folder):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(labels_folder, file_name)\n",
    "            image_name = file_name.replace(\".txt\", \".jpg\")  # Assuming the image format is .jpg\n",
    "            image_path = os.path.join(images_folder, image_name)\n",
    "\n",
    "            # Check if the image exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load the image\n",
    "            image = Image.open(image_path)\n",
    "            image_width, image_height = image.size\n",
    "\n",
    "            # Read and filter detections (get the highest confidence detection)\n",
    "            with open(file_path, 'r') as file:\n",
    "                detections = file.readlines()\n",
    "                detections = [line.strip().split() for line in detections]\n",
    "                detections = sorted(detections, key=lambda x: float(x[-1]), reverse=True)\n",
    "                best_detection = detections[0] if detections else None\n",
    "\n",
    "            if best_detection:\n",
    "                # Get bounding box coordinates\n",
    "                x_center, y_center, width, height = map(float, best_detection[1:5])\n",
    "                \n",
    "                # Convert normalized coordinates to actual image coordinates\n",
    "                x_min = int((x_center - width / 2) * image_width)\n",
    "                y_min = int((y_center - height / 2) * image_height)\n",
    "                x_max = int((x_center + width / 2) * image_width)\n",
    "                y_max = int((y_center + height / 2) * image_height)\n",
    "\n",
    "                # Crop the image to the odometer region\n",
    "                cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "                # Save or process the cropped image\n",
    "                output_image_path = os.path.join(output_folder, f\"cropped_{image_name}\")\n",
    "                cropped_image.save(output_image_path)\n",
    "                print(f\"Cropped image saved: {output_image_path}\")\n",
    "\n",
    "# Example usage\n",
    "results_folder = pwd + \"\\\\yolov5\\\\runs\\\\detect\\\\exp15\"  # Path to the experiment folder\n",
    "images_folder = pwd + \"\\\\CQ_CustomTest\"  # Path to your test images\n",
    "output_folder = pwd + \"\\\\CQ_CustomTest_cropped\"  # Folder where cropped images will be saved\n",
    "\n",
    "crop_odometer_regions(results_folder, images_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee07b9d-350a-4757-b711-dc061ed5b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stage 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dea7a7d-924e-43b2-81fe-1dc8db9cad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from io import BytesIO\n",
    "\n",
    "class OdometerDataset:\n",
    "    def __init__(self, root_folder, output_folder, transform=None):\n",
    "        self.root_folder = root_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.transform = transform\n",
    "        self.data = self.load_annotations()\n",
    "\n",
    "    def load_annotations(self):\n",
    "        data = []\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "        \n",
    "        for sub_folder in os.listdir(self.root_folder):\n",
    "            sub_folder_path = os.path.join(self.root_folder, sub_folder)\n",
    "            if not os.path.isdir(sub_folder_path):\n",
    "                continue\n",
    "\n",
    "            annotation_file = os.path.join(sub_folder_path, \"via_region_data.json\")\n",
    "            if not os.path.exists(annotation_file):\n",
    "                print(f\"Annotation file missing in {sub_folder_path}\")\n",
    "                continue\n",
    "\n",
    "            with open(annotation_file, 'r') as f:\n",
    "                annotations = json.load(f)\n",
    "\n",
    "            for item in annotations.values():\n",
    "                if 'filename' in item and 'regions' in item:\n",
    "                    image_path = item['filename']\n",
    "                    image_path = os.path.join(sub_folder_path, image_path) if not self.is_url(image_path) else image_path\n",
    "                    \n",
    "                    if not self.is_url(image_path) and not os.path.exists(image_path):\n",
    "                        print(f\"Skipping {image_path}: File not found or invalid.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Process each \"odometer\" region\n",
    "                    for region in item['regions']:\n",
    "                        if region['region_attributes']['identity'] == 'odometer':\n",
    "                            shape = region['shape_attributes']\n",
    "                            points = list(zip(shape['all_points_x'], shape['all_points_y']))\n",
    "                            x_min, y_min = min([point[0] for point in points]), min([point[1] for point in points])\n",
    "                            x_max, y_max = max([point[0] for point in points]), max([point[1] for point in points])\n",
    "                            \n",
    "                            # Load image, handle both local file and URL\n",
    "                            if self.is_url(image_path):\n",
    "                                response = requests.get(image_path)\n",
    "                                image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "                            else:\n",
    "                                image = Image.open(image_path).convert(\"RGB\")\n",
    "                            \n",
    "                            # Crop the image\n",
    "                            cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "                            \n",
    "                            # Get the \"odometer\" reading\n",
    "                            odometer_reading = region['region_attributes'].get('reading', 'N/A')\n",
    "\n",
    "                            # Save cropped image with the original filename in the output folder\n",
    "                            output_image_path = os.path.join(self.output_folder, os.path.basename(image_path))\n",
    "                            cropped_image.save(output_image_path)\n",
    "                            \n",
    "                            # Append data for future use (e.g., during training)\n",
    "                            data.append((output_image_path, odometer_reading))\n",
    "\n",
    "                            # If you want to display images\n",
    "                            # plt.figure(figsize=(5, 5))\n",
    "                            # plt.imshow(cropped_image)\n",
    "                            # plt.axis('off')\n",
    "                            # plt.title(f\"Odometer Reading: {odometer_reading}\")\n",
    "                            # plt.show()\n",
    "\n",
    "        return data\n",
    "\n",
    "    def is_url(self, path):\n",
    "        return path.startswith(\"http://\") or path.startswith(\"https://\")\n",
    "\n",
    "# Usage example\n",
    "root_folder = pwd + \"\\\\train\"  # Set this to your root folder path\n",
    "output_folder = pwd + \"\\\\cropped_train\"  # Set this to your output folder path\n",
    "dataset = OdometerDataset(root_folder, output_folder)\n",
    "\n",
    "# The `dataset.data` will have a list of tuples: (image_path, odometer_reading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bb0743d-0ef9-4e75-b04d-8ce563b5d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3659\n",
      "[('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_BvkovI_1654858778932.JPG', '39747'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_Z6oiQq_1654866593617.jpg', '010676'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_pvL8di_1654866596100.jpg', '75066'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_f7NYF2_1654866598642.jpg', '221498'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_B362xa_1654866600042.jpg', '76777'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_njD0k2_1654866602597.jpg', '091308'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_HdwE8p_1654866604942.jpg', '20818'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_v8Qx45_1654866607265.jpg', '114318'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_MiWioE_1654866608471.jpg', '56727'), ('C:\\\\Users\\\\sreek\\\\Downloads\\\\ClearQuote\\\\Assessment_2\\\\cropped_train\\\\scraped_vvB4vS_1654866609845.jpg', '038943')]\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.data))\n",
    "print(dataset.data[:10])\n",
    "dataset = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edce6ca5-69b6-43ae-b7d4-e324809f4814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved at trimmed_train_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the path for the output CSV file\n",
    "output_csv = \"trimmed_train_labels.csv\"\n",
    "\n",
    "# Write the dataset to a CSV file\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"path\", \"label\"])  # Write the header\n",
    "    writer.writerows(dataset)          # Write the dataset rows\n",
    "\n",
    "print(f\"CSV file saved at {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb584d0-1aa3-41b9-a690-bb152b001071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#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\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22906277-bb9a-4fd6-b8c3-d58f2fce04d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4848ebe-50bc-417e-b05e-15a5c1fabc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 544.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Set up the training loop\u001b[39;00m\n\u001b[0;32m     60\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Optimizer and Scheduler\u001b[39;00m\n\u001b[0;32m     64\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1327\u001b[0m         device,\n\u001b[0;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m         non_blocking,\n\u001b[0;32m   1330\u001b[0m     )\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 544.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Dataset Class\n",
    "class OdometerDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx, 0]\n",
    "        label = str(self.dataframe.iloc[idx, 1])  # Ensure label is a string (for OCR)\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image and label\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        labels = self.processor.tokenizer(label, padding=\"max_length\", truncation=True, max_length=16, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Transformation for training images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust this size based on the model input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize Processor and Model\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "\n",
    "# Set the required configuration values\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "train_dataset = OdometerDataset(dataframe=df, processor=processor, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Set up the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 5\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "              \n",
    "        # Now forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_odometer_model\")\n",
    "processor.save_pretrained(\"trained_odometer_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10cff891-d3ca-4399-88ec-1c99e84a4022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c69bebb-03fb-4be7-a1c4-11ed6e8fef8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 383.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Set up the training loop\u001b[39;00m\n\u001b[0;32m     60\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Optimizer and Mixed Precision Training\u001b[39;00m\n\u001b[0;32m     64\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-5\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3163\u001b[0m         )\n\u001b[1;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 900 (4 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 900\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[0;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[0;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1321\u001b[0m             device,\n\u001b[0;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1323\u001b[0m             non_blocking,\n\u001b[0;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1325\u001b[0m         )\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1327\u001b[0m         device,\n\u001b[0;32m   1328\u001b[0m         dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1329\u001b[0m         non_blocking,\n\u001b[0;32m   1330\u001b[0m     )\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 383.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(\"train_labels.csv\")\n",
    "\n",
    "# Dataset Class\n",
    "class OdometerDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.dataframe.iloc[idx, 0]\n",
    "        label = str(self.dataframe.iloc[idx, 1])  # Ensure label is a string (for OCR)\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process the image and label\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        labels = self.processor.tokenizer(label, padding=\"max_length\", truncation=True, max_length=16, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Transformation for training images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust this size based on the model input\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Initialize Processor and Model\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "\n",
    "# Set the required configuration values\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "\n",
    "# Prepare Dataset and DataLoader\n",
    "train_dataset = OdometerDataset(dataframe=df, processor=processor, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)  # Start with a smaller batch size\n",
    "\n",
    "# Set up the training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and Mixed Precision Training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()  # Gradient scaler for mixed precision training\n",
    "\n",
    "epochs = 5\n",
    "accumulation_steps = 4  # Accumulate gradients over multiple steps\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_odometer_model\")\n",
    "processor.save_pretrained(\"trained_odometer_processor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce20b29-eb72-4ae8-9b3b-01dfd5fe5493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEsting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3595c0d-2818-42f0-a8d6-30ca2b295fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR results saved to C:\\Users\\sreek\\Downloads\\ClearQuote\\Assessment_2\\ocr_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load TrOCR model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n",
    "\n",
    "# Function to perform OCR on an image\n",
    "def detect_text(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        detected_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return detected_text\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Load the .csv file\n",
    "csv_file = pwd + \"\\\\trimmed_train_labels.csv\"  # Replace with your .csv file path\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# Create a DataFrame with an additional column for detected text\n",
    "results = []\n",
    "for _, row in data.iterrows():\n",
    "    file_path = row[\"path\"]\n",
    "    label = row[\"label\"]\n",
    "    detected_text = detect_text(file_path)\n",
    "    results.append({\"name_of_file\": file_path, \"text_detected\": detected_text, \"original_label\": label})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "output_csv = pwd + \"\\\\ocr_results.csv\"\n",
    "df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"OCR results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf5b40-5fe1-42c3-9794-8d58062145d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
